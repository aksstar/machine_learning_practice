{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    CTRLLMHeadModel,\n",
    "    CTRLTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    TransfoXLLMHeadModel,\n",
    "    TransfoXLTokenizer,\n",
    "    XLMTokenizer,\n",
    "    XLMWithLMHeadModel,\n",
    "    XLNetLMHeadModel,\n",
    "    XLNetTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a33b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "    \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4d04b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aakashgouda/Python_Project/venv/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:902: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelWithLMHead, AutoTokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained('./models')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526b2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Hello World !\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "if encoded_prompt.size()[-1] == 0:\n",
    "    input_ids = None\n",
    "else:\n",
    "    input_ids = encoded_prompt\n",
    "    \n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=len(prompt_text) + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a754ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SEQUENCE 1 ===\n",
      "Professor McGonagall  was so furious with him she had to deal with him  after a week to think. <EOS><BOS> “Look at this — that,” said Professor McGonagall    Page | 578     Harry Potter and the Goblet of Fire\n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "Professor McGonagall  was so furious with him she had brought him along. <EOS><BOS> “Karkaroff agrees  to investigate.”   “You will not do, Harry Potter,” said Harry,  scowling at her. <EOS>\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "Professor McGonagall  was so furious with him she had to go up to her  room with Ron and Hermione and just to let her out of her way. <EOS><BOS> This time she came  out. <EOS><BOS> She turned, pointed the flask over to \n",
      "=== GENERATED SEQUENCE 4 ===\n",
      "Professor McGonagall  was so furious with him she had called him a “rapist” and she was  frantically trying to drag him off the grounds. <EOS><BOS> The Professor looked at the whole group and  saw Ron’s face. <EOS><BOS\n",
      "=== GENERATED SEQUENCE 5 ===\n",
      "Professor McGonagall  was so furious with him she had no use for it  from behind the curtains. <EOS><BOS> “Hands — Wands,” she asked. <EOS><BOS> “How do you think that thing can even look like a \n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Professor McGonagall  was so furious with him she had\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "if encoded_prompt.size()[-1] == 0:\n",
    "    input_ids = None\n",
    "else:\n",
    "    input_ids = encoded_prompt\n",
    "    \n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=len(prompt_text) + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=5,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "    # Decode text\n",
    "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "#     # Remove all text after the stop token\n",
    "#     text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "\n",
    "    # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "    total_sequence = (\n",
    "        prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "    )\n",
    "\n",
    "    generated_sequences.append(total_sequence)\n",
    "    print(total_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd79be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
